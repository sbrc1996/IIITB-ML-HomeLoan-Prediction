{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTING THE LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Importing Essential Libraries...')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "import sklearn\n",
    "%matplotlib inline\n",
    "\n",
    "print('Done!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading the data.....')\n",
    "df = pd.read_csv('./Data/train_data.csv')\n",
    "print('Done!!!')\n",
    "\n",
    "print('The first 5 rows are: ')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "print('The shape of the dataframe is:- Rows: ',df.shape[0],' Columns: ',df.shape[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the given data how many loans replayed and defaulted loans.\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "sns.countplot(x = 'TARGET',data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Who is the highest borrower? Male or Female?\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "sns.countplot(x='CODE_GENDER',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #How is the distribution of target labels? - Did most people return on time ?\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "sns.countplot(x ='TARGET',data=df, hue='TARGET',palette=\"Set1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Whether is it Female who has more difficulties or is it Male in repaying the loan?\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "sns.countplot(x='TARGET',hue='CODE_GENDER',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Who owns most number of the cars? M or F?\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "sns.countplot(x='CODE_GENDER', hue='FLAG_OWN_CAR', data=df,palette=\"Set1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Drawing Histogram!!!')\n",
    "for h in df.columns:\n",
    "    if df[h].nunique() < 100:\n",
    "        fig, ax = plt.subplots(1,1, figsize=(15, 6))\n",
    "        sns.histplot(df[h], palette='Blues_r')\n",
    "        fig.text(0.1, 0.95, f'{h}', fontsize=16, fontweight='bold', fontfamily='serif')\n",
    "        plt.xlabel('value ', fontsize=10)\n",
    "        plt.ylabel('count',fontsize=10)\n",
    "        plt.yticks(fontsize=13)\n",
    "        plt.box(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DROPPING THE COLS WITH 60% NULLS VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the columns that have > 60% NULL Values:\n",
    "to_drop=[] #this is a list that stores the names of cols having more than 50% nulls\n",
    "for features in df.columns:\n",
    "    percentage = (df[features].isna().sum()/df.shape[0]) *100\n",
    "    if df[features].isna().sum() > 0 and percentage > 60.0:\n",
    "        to_drop.append(features)\n",
    "        print(features,'    ' ,df[features].isna().sum(), percentage)\n",
    "        df.drop(features,axis=1,inplace=True)\n",
    "\n",
    "\n",
    "print('The shape of the dataframe is:- Rows: ',df.shape[0],' Columns: ',df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Check for duplicate data rows and drop the Id Column\n",
    "\n",
    "df.drop('SK_ID_CURR',axis= 1,inplace= True)\n",
    "\n",
    "countDuplicateRows = df[df.duplicated(subset = None, keep= False)].shape[0]\n",
    "print('The number of Duplicate Rows present here are: ',countDuplicateRows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REPLACE THE ABSURD VALUES BY NAN VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finding absurd values here.\n",
    "for i in df.columns:\n",
    "    if df[i].dtype == 'object':\n",
    "        print(\"Feature name: \",i,\" Unique values are: \",df[i].unique())\n",
    "\n",
    "## Replace them by nan values.\n",
    "df['CODE_GENDER'] = df['CODE_GENDER'].replace('XNA',np.nan)\n",
    "df['NAME_TYPE_SUITE'] = df['NAME_TYPE_SUITE'].replace('Other_A',np.nan)\n",
    "df['NAME_TYPE_SUITE'] = df['NAME_TYPE_SUITE'].replace('Other_B',np.nan)\n",
    "df['NAME_FAMILY_STATUS'] = df['NAME_FAMILY_STATUS'].replace('Unknown',np.nan)\n",
    "df['ORGANIZATION_TYPE'] = df['ORGANIZATION_TYPE'].replace('XNA',np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking for the NULL if it is still existing.\n",
    "for i in df.columns:\n",
    "    if df[i].isna().sum() > 0:\n",
    "        print(i,df[i].dtype,df[i].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing the NULL values from the categorical data:\n",
    "\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# impt = SimpleImputer(strategy= 'most_frequent')\n",
    "\n",
    "for i in df.columns:\n",
    "    if df[i].dtype == 'object' and df[i].isna().sum() > 0:\n",
    "        df[i] = df[i].fillna(df[i].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing the null values for the non-categorical data: (Median as it may contain some outliers.)\n",
    "\n",
    "for i in df.columns:\n",
    "    if df[i].dtype != 'object':\n",
    "        df[i] = df[i].fillna(int(df[i].median()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHANGING THE -VE VALUES TO +VE FOR BETTER READABILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing the negative values\n",
    "#DAYS_BIRTH DAYS_EMPLOYED DAYS_REGISTRATION DAYS_ID_PUBLISH\n",
    "\n",
    "df['DAYS_BIRTH'] = df['DAYS_BIRTH'].abs()\n",
    "df['DAYS_EMPLOYED'] = df['DAYS_EMPLOYED'].abs()\n",
    "df['DAYS_REGISTRATION'] = df['DAYS_REGISTRATION'].abs()\n",
    "df['DAYS_ID_PUBLISH'] = df['DAYS_ID_PUBLISH'].abs() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIVIDE INTO X AND Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Break the dataset into X and Y\n",
    "x = df.drop('TARGET',axis= 1)\n",
    "y = df['TARGET']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for the Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting BoxPlot of the left out columns to check if they have any outliers.\n",
    "print('Before Removing Outliers')\n",
    "count = 0\n",
    "for i in x.columns:\n",
    "    fig = plt.figure(figsize=(7,4))\n",
    "    plt.boxplot(x[i])\n",
    "    plt.suptitle(i)\n",
    "    plt.show()\n",
    "    count += 1\n",
    "\n",
    "print('Total Boxplots printed are: ',count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REPLACE THE IQR BY CAPPING IT WITH LOWER AND UPPER QUARTILE VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IQR method to display outliers if present.\n",
    "\n",
    "for i in x.columns:\n",
    "    if x[i].dtype != 'object':\n",
    "        Q1 = x[i].quantile(0.25)\n",
    "        Q3 = x[i].quantile(0.75)  \n",
    "        IQR = Q3 - Q1\n",
    "        ll = Q1 - (IQR*1.5)\n",
    "        ul = Q3 + (IQR*1.5)\n",
    "\n",
    "        l = x[i].loc[x[i] < ll].to_list()\n",
    "        u = x[i].loc[x[i] > ul].to_list()\n",
    "        \n",
    "        #capping the outliers by the lower quartile and upper quartile\n",
    "        \n",
    "        #x[i][(x[i]>ul) | (x[i]<ll)] = x[i].median()\n",
    "        x[i]=np.where(x[i]>ul,ul,np.where(x[i]<ll,ll,x[i])) \n",
    "        \n",
    "        \n",
    "\n",
    "        ln = x[i].loc[x[i] < ll].to_list()\n",
    "        un = x[i].loc[x[i] > ul].to_list()\n",
    "\n",
    "        \n",
    "        print(i,Q1,Q3,IQR,ll,ul,len(l),len(u),len(l)+len(u),len(ln),len(un),len(ln)+len(un)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting BoxPlot of the left out columns after removal of outliers.\n",
    "print('After Removing Outliers')\n",
    "count = 0\n",
    "for i in x.columns:\n",
    "    fig = plt.figure(figsize=(7,4))\n",
    "    plt.boxplot(x[i])\n",
    "    plt.suptitle(i)\n",
    "    plt.show()\n",
    "    count += 1\n",
    "\n",
    "print('Total Boxplots printed are: ',count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LABEL ENCODING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Label Encode the Object columns:\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "print('Applying Label Encoding....')\n",
    "for i in x.columns:\n",
    "    if x[i].dtype == 'object':\n",
    "        x[i] = label_encoder.fit_transform(x[i])\n",
    "print('Done!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OVERSAMPLING USING THE ADASYN TO CREATE SYNTHETIC DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Oversampling using ADASYN and other SMOTE Techniques.\n",
    "from collections import Counter\n",
    "import imblearn\n",
    "from imblearn.over_sampling import KMeansSMOTE,SMOTE,ADASYN,SVMSMOTE\n",
    "adasyn = ADASYN(0.75,random_state=30)\n",
    "X_res,Y_res = adasyn.fit_resample(x,y)\n",
    "\n",
    "\n",
    "print(\"The number of classes before fit {}\",format(Counter(y)))\n",
    "print(\"The number of classes after fit {}\",format(Counter(Y_res)))\n",
    "\n",
    "print('Shape before sampling',x.shape,y.shape)\n",
    "print('Shape after sampling',X_res.shape,Y_res.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN TEST SPLIT THE ENTIRE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Test Split the Training Data.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_Train,X_Test,Y_Train,Y_Test = train_test_split(X_res,Y_res,train_size= 0.7,shuffle=True,random_state=30)\n",
    "print(\"Training Data shape:  \",X_Train.shape,Y_Train.shape)\n",
    "print(\"Testing  Data shape:  \",X_Test.shape,Y_Test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE SELECTION USING RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "# For Random Forest Classifier\n",
    "sel = SelectFromModel(RandomForestClassifier(n_estimators= 100,criterion= 'entropy'))\n",
    "sel.fit(X_Train,Y_Train)\n",
    "\n",
    "\n",
    "# # For Decision Tree Classifier\n",
    "# sel = SelectFromModel(ExtraTreesClassifier(n_estimators= 100,criterion= 'gini'))\n",
    "# sel.fit(X_Train,Y_Train)\n",
    "\n",
    "\n",
    "selected_features = X_Train.columns[(sel.get_support())]\n",
    "print(\"The Number of features selected are: \",len(selected_features))\n",
    "print(\"The features selected are: \",selected_features)\n",
    "\n",
    "# pd.series(sel.estimator_,feature_importa).hist()\n",
    "\n",
    "#Dropping the columns that are not present in the selected_features list\n",
    "\n",
    "for i in X_Train.columns:\n",
    "    if i not in selected_features:\n",
    "        X_Train.drop(i,axis= 1,inplace= True)\n",
    "        X_Test.drop(i,axis= 1,inplace= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STORE THE VALUES FOR REPRODUCING THE SAME IN TEST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Storing the results of the columns left out.\n",
    "selected_features = []\n",
    "for i in X_Train.columns:\n",
    "    selected_features.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPLYING SCALING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying Standard Scalar on the entire dataset.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print('Applying Scaling on the training data only for the features...')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_Train)\n",
    "scaler.fit(X_Test)\n",
    "X_Train = scaler.transform(X_Train)\n",
    "X_Test = scaler.transform(X_Test)\n",
    "print('Done!!')\n",
    "#Pass this scaled data as input to the Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTING METRICS FOR COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,confusion_matrix,classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTING ML CLASSIFICATION MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_Regression  = LogisticRegression(random_state= 30,max_iter=10000)\n",
    "\n",
    "logistic_Regression.fit(X_Train,Y_Train)\n",
    "Y_Pred = logistic_Regression.predict(X_Test)\n",
    "\n",
    "lacc = accuracy_score(Y_Pred,Y_Test)\n",
    "lf1 = f1_score(Y_Pred,Y_Test)\n",
    "lauc_score = roc_auc_score(Y_Pred,Y_Test)\n",
    "print('The accuracy of the model on training data is: ')\n",
    "\n",
    "print('The accuracy  is: ',lacc*100,'%')\n",
    "print('The value of f1_score is: ',lf1*100,'%')\n",
    "print('The value of Roc AUC Score is: ',lauc_score*100,'%')\n",
    "\n",
    "print(\"The confusion matrix is: \\n\\n\",confusion_matrix(Y_Test,Y_Pred))\n",
    "print(\"The classification report is: \\n\\n\",classification_report(Y_Test,Y_Pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k NEAREST NEIGHBOURS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIND THE BEST VALUE OF K HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# for k in range(1, 100, 5):\n",
    "#     k = k + 1\n",
    "#     knn = KNeighborsClassifier(n_neighbors = k).fit(X_Train,Y_Train)\n",
    "#     acc = knn.score(X_Test,Y_Test)\n",
    "#     print(\"Accuracy for k = \",k,\" is: \",acc)\n",
    "\n",
    "# Here we are selecting which is the best n value for the KNN algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USE THE BEST VALUE TO FIND THE ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors= 2).fit(X_Train,Y_Train)\n",
    "Y_Pred = knn.predict(X_Test)\n",
    "kacc = accuracy_score(Y_Pred,Y_Test)\n",
    "kf1 = f1_score(Y_Pred,Y_Test)\n",
    "kauc_score = roc_auc_score(Y_Pred,Y_Test)\n",
    "\n",
    "\n",
    "print('The accuracy of the model on Data is: ')\n",
    "\n",
    "print('The accuracy  is: ',kacc*100,'%')\n",
    "print('The value of f1_score is: ',kf1*100,'%')\n",
    "print('The value of Roc AUC Score is: ',kauc_score*100,'%')\n",
    "\n",
    "print(\"The confusion matrix is: \\n\\n\",confusion_matrix(Y_Test,Y_Pred))\n",
    "print(\"The classification report is: \\n\\n\",classification_report(Y_Test,Y_Pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tried this but this took a huge amount of time. Like 6 hours.\n",
    "\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# svc = SVC()\n",
    "# svc.fit(X_Train,Y_Train)\n",
    "\n",
    "# Y_Pred = svc.predict(X_Test)\n",
    "# acc = accuracy_score(Y_Pred,Y_Test)\n",
    "# f1 = f1_score(Y_Pred,Y_Test)\n",
    "# auc_score = roc_auc_score(Y_Pred,Y_Test)\n",
    "\n",
    "\n",
    "# print('The accuracy of the model on Data is: ')\n",
    "\n",
    "# print('The accuracy  is: ',acc*100,'%')\n",
    "# print('The value of f1_score is: ',f1*100,'%')\n",
    "# print('The value of Roc AUC Score is: ',auc_score*100,'%')\n",
    "\n",
    "# print(\"The confusion matrix is: \\n\\n\",confusion_matrix(Y_Test,Y_Pred))\n",
    "# print(\"The classification report is: \\n\\n\",classification_report(Y_Test,Y_Pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG BOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYPERPARAMETER TUNING USING GRIDSEARCHCV TO GET THE BEST PARAMETERS FOR XG BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "# xgb_model = XGBClassifier(random_state = 30)\n",
    "# search_space = {\n",
    "#     \"n_estimators\" :   [100,200],\n",
    "#     \"max_depth\" :      [3,6,7],\n",
    "#     \"gamma\" :          [0.01,0.1],\n",
    "#     \"learning_rate\" :  [0.001,0.01,0.1,1]\n",
    "# }\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# GS = GridSearchCV(\n",
    "#     estimator= xgb_model,\n",
    "#     param_grid= search_space,\n",
    "#     scoring= [\"roc_auc\",\"roc_auc_ovr\",\"roc_auc_ovo\",\"f1\",\"f1_micro\",\"f1_macro\",\"accuracy\"],\n",
    "#     refit= \"roc_auc\",\n",
    "#     cv= 5,\n",
    "#     verbose= 4\n",
    "# )\n",
    "\n",
    "# GS.fit(X_Train,Y_Train)\n",
    "\n",
    "# print(\"The best estimator is: \",GS.best_estimator_)\n",
    "# print(\"The best parameter is: \",GS.best_params_)\n",
    "# print(\"The best AUC_ROC score is: \",GS.best_score_)\n",
    "# df_XGBoost = pd.DataFrame(GS.cv_results_)\n",
    "# df_XGBoost = df_XGBoost.sort_values(\"rank_test_roc_auc\")\n",
    "# df_XGBoost.to_csv('./Test_Output/XGBoost_GridSearchCV.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPLYING THE BEST PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model_XGB = XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
    "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
    "              early_stopping_rounds=None, enable_categorical=False,\n",
    "              eval_metric=None, feature_types=None, gamma=0.1, gpu_id=-1,\n",
    "              grow_policy='depthwise', importance_type=None,\n",
    "              interaction_constraints='', learning_rate=0.1, max_bin=256,\n",
    "              max_cat_threshold=64, max_cat_to_onehot=4, max_delta_step=0,\n",
    "              max_depth=7, max_leaves=0, min_child_weight=1,\n",
    "              monotone_constraints='()', n_estimators=200, n_jobs=0,\n",
    "              num_parallel_tree=1, predictor='auto', random_state=30)\n",
    "\n",
    "model_XGB.fit(X_Train,Y_Train)\n",
    "\n",
    "Y_Pred = model_XGB.predict(X_Test)\n",
    "xacc = accuracy_score(Y_Pred,Y_Test)\n",
    "xf1 = f1_score(Y_Pred,Y_Test)\n",
    "xauc_score = roc_auc_score(Y_Pred,Y_Test)\n",
    "\n",
    "\n",
    "print('The accuracy of the model on Data is: ')\n",
    "\n",
    "print('The accuracy  is: ',xacc*100,'%')\n",
    "print('The value of f1_score is: ',xf1*100,'%')\n",
    "print('The value of Roc AUC Score is: ',xauc_score*100,'%')\n",
    "\n",
    "print(\"The confusion matrix is: \\n\\n\",confusion_matrix(Y_Test,Y_Pred))\n",
    "print(\"The classification report is: \\n\\n\",classification_report(Y_Test,Y_Pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECISION TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state = 30)\n",
    "dt.fit(X_Train,Y_Train)\n",
    "Y_Pred = dt.predict(X_Test)\n",
    "dacc = accuracy_score(Y_Pred,Y_Test)\n",
    "df1 = f1_score(Y_Pred,Y_Test)\n",
    "dauc_score = roc_auc_score(Y_Pred,Y_Test)\n",
    "\n",
    "\n",
    "print('The accuracy of the model on Data is: ')\n",
    "\n",
    "print('The accuracy  is: ',dacc*100,'%')\n",
    "print('The value of f1_score is: ',df1*100,'%')\n",
    "print('The value of Roc AUC Score is: ',dauc_score*100,'%')\n",
    "\n",
    "print(\"The confusion matrix is: \\n\\n\",confusion_matrix(Y_Test,Y_Pred))\n",
    "print(\"The classification report is: \\n\\n\",classification_report(Y_Test,Y_Pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state = 30).fit(X_Train,Y_Train)\n",
    "Y_Pred = rf.predict(X_Test)\n",
    "racc = accuracy_score(Y_Pred,Y_Test)\n",
    "rf1 = f1_score(Y_Pred,Y_Test)\n",
    "rauc_score = roc_auc_score(Y_Pred,Y_Test)\n",
    "\n",
    "\n",
    "print('The accuracy of the model on Data is: ')\n",
    "\n",
    "print('The accuracy  is: ',racc*100,'%')\n",
    "print('The value of f1_score is: ',rf1*100,'%')\n",
    "print('The value of Roc AUC Score is: ',rauc_score*100,'%')\n",
    "\n",
    "print(\"The confusion matrix is: \\n\\n\",confusion_matrix(Y_Test,Y_Pred))\n",
    "print(\"The classification report is: \\n\\n\",classification_report(Y_Test,Y_Pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAUSSIAN NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##GAUSSIAN NAIVE BAYES\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_Train,Y_Train)\n",
    "Y_Pred = model.predict(X_Test)\n",
    "gnbacc = accuracy_score(Y_Pred,Y_Test)\n",
    "gnbf1 = f1_score(Y_Pred,Y_Test)\n",
    "gnbauc_score = roc_auc_score(Y_Pred,Y_Test)\n",
    "\n",
    "\n",
    "print('The accuracy of the model on Data is: ')\n",
    "\n",
    "print('The accuracy  is: ',gnbacc*100,'%')\n",
    "print('The value of f1_score is: ',gnbf1*100,'%')\n",
    "print('The value of Roc AUC Score is: ',gnbauc_score*100,'%')\n",
    "\n",
    "print(\"The confusion matrix is: \\n\\n\",confusion_matrix(Y_Test,Y_Pred))\n",
    "print(\"The classification report is: \\n\\n\",classification_report(Y_Test,Y_Pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERNOULLI NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BERNOULLI NAIVE BAYES\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(X_Train,Y_Train)\n",
    "Y_Pred = model.predict(X_Test)\n",
    "bnbacc = accuracy_score(Y_Pred,Y_Test)\n",
    "bnbf1 = f1_score(Y_Pred,Y_Test)\n",
    "bnbauc_score = roc_auc_score(Y_Pred,Y_Test)\n",
    "\n",
    "\n",
    "print('The accuracy of the model on Data is: ')\n",
    "\n",
    "print('The accuracy  is: ',bnbacc*100,'%')\n",
    "print('The value of f1_score is: ',bnbf1*100,'%')\n",
    "print('The value of Roc AUC Score is: ',bnbauc_score*100,'%')\n",
    "\n",
    "print(\"The confusion matrix is: \\n\\n\",confusion_matrix(Y_Test,Y_Pred))\n",
    "print(\"The classification report is: \\n\\n\",classification_report(Y_Test,Y_Pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as ltb\n",
    "model = ltb.LGBMClassifier()\n",
    "# dt = DecisionTreeClassifier()\n",
    "model.fit(X_Train,Y_Train)\n",
    "Y_Pred = model.predict(X_Test)\n",
    "\n",
    "lgmacc = accuracy_score(Y_Pred,Y_Test)\n",
    "lgmf1 = f1_score(Y_Pred,Y_Test)\n",
    "lgmauc_score = roc_auc_score(Y_Pred,Y_Test)\n",
    "\n",
    "\n",
    "print('The accuracy of the model on Data is: ')\n",
    "\n",
    "print('The accuracy  is: ',lgmacc*100,'%')\n",
    "print('The value of f1_score is: ',lgmf1*100,'%')\n",
    "print('The value of Roc AUC Score is: ',lgmauc_score*100,'%')\n",
    "\n",
    "print(\"The confusion matrix is: \\n\\n\",confusion_matrix(Y_Test,Y_Pred))\n",
    "print(\"The classification report is: \\n\\n\",classification_report(Y_Test,Y_Pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL COMPARISON AND PLOTTING THEM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "t = PrettyTable(['ALGO','ACCURACY SCORE','F1 SCORE','ROC-AUC SCORE'])\n",
    "t.add_row(['Logistic Regression',round(lacc,2),round(lf1,2),round(lauc_score,2)])\n",
    "t.add_row(['KNN',round(kacc,2),round(kf1,2),round(kauc_score,2)])\n",
    "t.add_row(['XG Boost',round(xacc,2),round(xf1,2),round(xauc_score,2)])\n",
    "t.add_row(['Decision Trees',round(dacc,2),round(df1,2),round(dauc_score,2)])\n",
    "t.add_row(['Random Forest',round(racc,2),round(rf1,2),round(rauc_score,2)])\n",
    "t.add_row(['Gaussian NB', round(gnbacc,2),round(gnbf1,2),round(gnbauc_score,2)])\n",
    "t.add_row(['Bernoulli NB',round(bnbacc,2),round(bnbf1,2),round(bnbauc_score,2)])\n",
    "t.add_row(['Light GBM',round(lgmacc,2),round(lgmf1,2),round(lgmauc_score,2)])\n",
    "\n",
    "\n",
    "print(t)\n",
    "\n",
    "#Looking at the values it can be concluded that the best algo is XGBOOST!!!!\n",
    "x_axis = np.array([\"Logistic Regression\",\"\\nKNN\",\"XG Boost\",\"\\nDecision Trees\",\"Random Forest\",\"\\nGaussian NB\",\"Bernoulli NB\",\"\\nLight GBM\"])\n",
    "y_axis = np.array([lauc_score,kauc_score,xauc_score,dauc_score,rauc_score,gnbauc_score,bnbauc_score,lgmauc_score])\n",
    "plt.bar(x_axis,y_axis,align= 'center',width= 0.8)\n",
    "plt.show()\n",
    "\n",
    "## Hence use XG Boost!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOADING THE TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing the test data.\n",
    "\n",
    "print('Loading the test data.....')\n",
    "df_test = pd.read_csv('./Data/test_data.csv')\n",
    "print('Done!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()\n",
    "df_test.info()\n",
    "\n",
    "countDuplicateRows = df_test[df_test.duplicated(subset = None, keep= False)].shape[0]\n",
    "print('The number of Duplicate Rows present here are: ',countDuplicateRows)\n",
    "\n",
    "id_column = df_test['SK_ID_CURR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING IN TEST DATA SAME AS THAT OF TRAIN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace the absurd values by nan values.\n",
    "df_test['CODE_GENDER'] = df_test['CODE_GENDER'].replace('XNA',np.nan)\n",
    "df_test['NAME_TYPE_SUITE'] = df_test['NAME_TYPE_SUITE'].replace('Other_A',np.nan)\n",
    "df_test['NAME_TYPE_SUITE'] = df_test['NAME_TYPE_SUITE'].replace('Other_B',np.nan)\n",
    "df_test['NAME_FAMILY_STATUS'] = df_test['NAME_FAMILY_STATUS'].replace('Unknown',np.nan)\n",
    "df_test['ORGANIZATION_TYPE'] = df_test['ORGANIZATION_TYPE'].replace('XNA',np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing the negative values\n",
    "#DAYS_BIRTH DAYS_EMPLOYED DAYS_REGISTRATION DAYS_ID_PUBLISH\n",
    "\n",
    "df_test['DAYS_BIRTH'] = df_test['DAYS_BIRTH'].abs()\n",
    "df_test['DAYS_EMPLOYED'] = df_test['DAYS_EMPLOYED'].abs()\n",
    "df_test['DAYS_REGISTRATION'] = df_test['DAYS_REGISTRATION'].abs()\n",
    "df_test['DAYS_ID_PUBLISH'] = df_test['DAYS_ID_PUBLISH'].abs() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all the columns that are not in X_Train\n",
    "for i in df_test.columns:\n",
    "    if i not in selected_features:\n",
    "        df_test.drop(i,axis=1,inplace= True)\n",
    "\n",
    "print(df_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing the NULL values from object data type.\n",
    "for i in df_test.columns:\n",
    "    if df_test[i].dtype == 'object' and df_test[i].isna().sum() > 0:\n",
    "        df_test[i] = df_test[i].fillna(df_test[i].mode()[0])\n",
    "\n",
    "## Removing the NULL values from Non Object data type.\n",
    "for i in df_test.columns:\n",
    "    if df_test[i].dtype != 'object':\n",
    "        df_test[i] = df_test[i].fillna(int(df_test[i].median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Capping  Outliers using IQR Method.\n",
    "\n",
    "for i in df_test.columns:\n",
    "    if df_test[i].dtype != 'object':\n",
    "        Q1 = x[i].quantile(0.25)\n",
    "        Q3 = x[i].quantile(0.75)  \n",
    "        IQR = Q3 - Q1\n",
    "        ll = Q1 - IQR*1.5\n",
    "        ul = Q3 + IQR*1.5\n",
    "         \n",
    "        #replace the outliers by the median\n",
    "        \n",
    "        #df_test[i][(df_test[i]>ul) | (df_test[i]<ll)] = df_test[i].median()\n",
    "        df_test[i]=np.where(df_test[i]>ul,ul,np.where(df_test[i]<ll,ll,df_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Label Encode the Object columns:\n",
    "\n",
    "\n",
    "print('Applying Label Encoding....')\n",
    "for i in df_test.columns:\n",
    "    if df_test[i].dtype == 'object':\n",
    "        df_test[i] = label_encoder.fit_transform(df_test[i])\n",
    "print('Done!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying Standard Scalar on the entire dataset.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print('Applying Scaling on the training data only for the features...')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(df_test)\n",
    "\n",
    "df_Test = scaler.transform(df_test)\n",
    "print('Done!!')\n",
    "#Pass this scaled data as input to the Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the XG boost algo to get Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying the best model here.\n",
    "Y_Pred_final = model_XGB.predict(df_test)\n",
    "\n",
    "# Here Y_Pred_final is the final data prepared by us now we need to upload it to  kaggle.\n",
    "\n",
    "\n",
    "print(type(Y_Pred_final))\n",
    "print(type(id_column))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING CSV FILE FOR THE KAGGLE SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating CSV for kaggle\n",
    "\n",
    "df_kaggle = id_column.to_frame()\n",
    "\n",
    "df_kaggle['TARGET'] = Y_Pred_final.tolist()\n",
    "\n",
    "df_kaggle.to_csv('./Test_Output/Submission_new_XGB.csv',index= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                # ---------------------------------------------- THE END ----------------------------------------------#  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov  2 2022, 18:53:38) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
